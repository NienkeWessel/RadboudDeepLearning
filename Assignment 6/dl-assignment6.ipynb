{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning &mdash; Assignment 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sixth assignment for the 2020 Deep Learning course (NWI-IMC058) of the Radboud University.\n",
    "\n",
    "_Twan van Laarhoven (tvanlaarhoven@cs.ru.nl) and Gijs van Tulder (g.vantulder@cs.ru.nl)_\n",
    "\n",
    "_October 2020_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "**Names:**\n",
    "\n",
    "**Group:**\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "* Fill in your names and the name of your group.\n",
    "* Answer the questions and complete the code where necessary.\n",
    "* Re-run the whole notebook before you submit your work.\n",
    "* Save the notebook as a PDF and submit that in Brightspace together with the `.ipynb` notebook file.\n",
    "* The easiest way to make a PDF of your notebook is via File > Print Preview and then use your browser's print option to print to PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "In this assignment you will\n",
    "1. Build a variational auto encoder\n",
    "2. Extend the model to a conditional VAE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required software\n",
    "\n",
    "As before you will need these libraries:\n",
    "* `torch` and `torchvision` for PyTorch,\n",
    "* `d2l`, the library that comes with [Dive into deep learning](https://d2l.ai) book.\n",
    "\n",
    "All libraries can be installed with `pip install`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will use the MNIST digit dataset. This dataset consists of 28×28 binary images and has 60000 training examples divded over 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = {'batch_size':32, 'shuffle':True}\n",
    "train_iter = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()),\n",
    "    **opts)\n",
    "test_iter = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
    "    **opts)\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the code to load the MNIST dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Variational Auto Encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a Variational Auto Encoder. This model consists of two networks, an encoder and a decoder.\n",
    "The encoder produces a distribution in the latent space, represented as the parameters of a normal distribution. The decoder takes the latent space representation and produces an output in the data space.\n",
    "\n",
    "**Complete the implementation below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_size=2):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        # Components of the encoder network\n",
    "        self.encoder_part1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=2), nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7*7*64, 16), nn.ReLU()\n",
    "        )\n",
    "        self.encoder_mean   = nn.Linear(16, latent_size)\n",
    "        self.encoder_logvar = nn.Linear(16, latent_size)\n",
    "        \n",
    "        # Components of the decoder\n",
    "        self.decoder_part1_z = nn.Linear(latent_size, 7*7*64)\n",
    "        self.decoder_part2 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1, output_padding=1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, padding=1, output_padding=1, stride=2),\n",
    "            # TODO: Choose an appropriate activation function for the final layer.\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder_part1(x)\n",
    "        return self.encoder_mean(h), self.encoder_logvar(h)\n",
    "\n",
    "    def sample_latent(self, mean_z, logvar_z):\n",
    "        std_z = torch.exp(0.5*logvar_z)\n",
    "        eps = torch.randn_like(std_z)\n",
    "        # TODO: turn the sample ε from N(0,1) into a sample from N(μ,σ)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.decoder_part1_z(z)\n",
    "        h = torch.reshape(h, (-1,64,7,7)) # Unflatten\n",
    "        return self.decoder_part2(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_z, logvar_z = self.encode(x)\n",
    "        z = self.sample_latent(mean_z, logvar_z)\n",
    "        return self.decode(z), mean_z, logvar_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder produces two outputs that together give the parameters of a normal distribution: mean and logvar, so $\\mu$ and $\\log(\\sigma^2)$. The latter might seem strange, but there is a good reason for it.  \n",
    "**What can go wrong if the decoder network directly outputs σ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss for a variational autoencoder consists of two parts:\n",
    "1. The reconstruction loss, which is the log likelihood of the data,\n",
    "$L_\\text{R} = P(x\\mid z)$.\n",
    "2. The Kulback-Leibler divergence from the encoder output to the target distribution,\n",
    "$L_\\text{KL}= KL(Q(z)\\| P(z))$.\n",
    "\n",
    "In our case the data is binary, so we use binary cross entropy for the reconstruction loss.\n",
    "\n",
    "The derivation of the KL loss term can be found in appendix B of the VAE paper; [Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014](https://arxiv.org/pdf/1312.6114.pdf)\n",
    "\n",
    "**Implement the KL loss term below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(recon_x, x):\n",
    "    return F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "def kl_loss(mean_z, logvar_z):\n",
    "    # TODO: your code here\n",
    "    pass\n",
    "\n",
    "def loss_function(recon_x, x, mean_z, logvar_z):\n",
    "    l_r = reconstruction_loss(recon_x, x)\n",
    "    l_kl = kl_loss(mean_z, logvar_z)\n",
    "    return l_r + l_kl, l_r, l_kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Training our VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete the training loop below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs = 10):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[0, num_epochs],\n",
    "                            legend=['train loss', 'test loss'])\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = d2l.Accumulator(3)\n",
    "        model.train()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            X = X.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # TODO: compute the outputs and loss\n",
    "            # Optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Track our progress\n",
    "            metric.add(loss_r, loss_kl, X.shape[0])\n",
    "            train_loss_r  = metric[0] / metric[2]\n",
    "            train_loss_kl = metric[1] / metric[2]\n",
    "            train_loss = train_loss_r + train_loss_kl\n",
    "            if i > 0 and i % 50 == 0:\n",
    "                animator.add(epoch + i / len(train_iter), (train_loss, None))\n",
    "        test_loss_r, test_loss_kl = test(model)\n",
    "        test_loss = test_loss_r + test_loss_kl\n",
    "        animator.add(epoch+1, (None, test_loss))\n",
    "    print(f'training loss {train_loss:.3f}, test loss {test_loss:.3f}')\n",
    "    print(f'training reconstruction loss {train_loss_r:.3f}, test reconstruction loss {test_loss_r:.3f}')\n",
    "    print(f'training KL loss {train_loss_kl:.3f}, test KL loss {test_loss_kl:.3f}')\n",
    "\n",
    "\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    metric = d2l.Accumulator(3)\n",
    "    with torch.no_grad():\n",
    "        for i, (X, y) in enumerate(test_iter):\n",
    "            X = X.to(device)\n",
    "            # TODO: compute the outputs and loss\n",
    "            metric.add(loss_r, loss_kl, X.shape[0])\n",
    "    test_loss_r  = metric[0] / metric[2]\n",
    "    test_loss_kl = metric[1] / metric[2]\n",
    "    return test_loss_r, test_loss_kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = d2l.try_gpu()\n",
    "model = VAE().to(device)\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you increase the numer of latent dimensions, how does that affect the reconstruction loss and the KL loss terms?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Visualizing the latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the function below to visualize the 2D latent space, by running the decoder on $z$ values sampled at regular intervals.\n",
    "\n",
    "**Complete the code below and run it to plot the latent space.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent(model):\n",
    "    # display a n*n 2D manifold of digits\n",
    "    n = 30\n",
    "    digit_size = 28\n",
    "    scale = 2.0\n",
    "    figsize = 10\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-scale, scale, n)\n",
    "    grid_y = np.linspace(-scale, scale, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            # TODO: run the decoder on z = [xi,yi].\n",
    "            x_decoded = ...\n",
    "            figure[\n",
    "                i * digit_size : (i + 1) * digit_size,\n",
    "                j * digit_size : (j + 1) * digit_size,\n",
    "            ] = x_decoded.detach().cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range + 1\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_latent(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Would it be possible to classify digits based on this latent representation?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you retrain the model, would you expect the latent space to look the same. If not, what differences can you expect?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of visualizing the latent space is by making a scatter plot of the training data in the latent space.\n",
    "\n",
    "**Complete and run the code below to make a scatterplot of the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplot_latent(model):\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    zs, ys = [], []\n",
    "    for i, (X, y) in enumerate(train_iter):\n",
    "        if i > 100: # Limit to a few batches to save memory\n",
    "            break\n",
    "        # TODO: compute mean z\n",
    "        z_mean = ...\n",
    "        zs.append(z_mean.detach().cpu())\n",
    "        ys.append(y)\n",
    "    zs = torch.cat(zs).numpy()\n",
    "    ys = torch.cat(ys).numpy()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(zs[:, 0], zs[:, 1], c=ys)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.show()\n",
    "\n",
    "scatterplot_latent(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare this figure to the previous one, what do you notice?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Look at the distribution of the data in the latent space. What distribution do you expect it to have based on the theory? Does the plot match this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Conditional Variational Autoencoder\n",
    "\n",
    "An extension of variational autoencoders uses labels to *condition* the encoder and decoder models.\n",
    "In this conditional VAE, the decoder becomes $P(x|z,y)$ and the encoder $Q(z|x,y)$.\n",
    "In practice this means that the label $y$ is given as an extra input to the both the encoder and the decoder.\n",
    "\n",
    "For details see the paper [Semi-Supervised Learning with Deep Generative Models; Kingma, Rezende, Mohamed, Welling; 2014](https://arxiv.org/pdf/1406.5298.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the labels in the decoder, we can concatenate the label with the latent vector. Or equivalently, we can use separate weights for $z$ and $y$ in the first layer, so that layer computes $W_z * z + W_y * y + b$.\n",
    "\n",
    "Similarly for the encoder, except there we will still use a convolutional layer for $x$, combined with a fully connected layer for $y$.\n",
    "\n",
    "**Complete the implementation of the conditional VAE below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, latent_size=2, num_classes=10):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        # Components of the encoder network\n",
    "        # TODO: split the first layer from the previous encoder network into a separate variable,\n",
    "        #       and add a layer to use with the y input\n",
    "        self.encoder_part1_x = ...\n",
    "        self.encoder_part1_y = ...\n",
    "        self.encoder_part2 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=2), nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7*7*64, 16), nn.ReLU()\n",
    "        )\n",
    "        self.encoder_mean   = nn.Linear(16, latent_size)\n",
    "        self.encoder_logvar = nn.Linear(16, latent_size)\n",
    "\n",
    "        # Components of the decoder network\n",
    "        self.decoder_part1_z = nn.Linear(latent_size, 7*7*64)\n",
    "        # TODO: add layer to use with the y input\n",
    "        self.decoder_part2 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1, output_padding=1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, padding=1, output_padding=1, stride=2),\n",
    "            # TODO: see VAE\n",
    "        )\n",
    "\n",
    "    def encode(self, x, y):\n",
    "        h = self.encoder_part1_x(x) + self.encoder_part1_y(y).reshape(-1,32,14,14)\n",
    "        h = self.encoder_part2(h)\n",
    "        return self.encoder_mean(h), self.encoder_logvar(h)\n",
    "\n",
    "    def sample_latent(self, mean_z, logvar_z):\n",
    "        std_z = torch.exp(0.5*logvar_z)\n",
    "        eps = torch.randn_like(std_z)\n",
    "        # TODO: see VAE\n",
    "\n",
    "    def decode(self, z, y):\n",
    "        # TODO: use a first layer that combines z and y\n",
    "        h = ...\n",
    "        h = torch.reshape(h, (-1,64,7,7))\n",
    "        return self.decoder_part2(h)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        mean_z, logvar_z = self.encode(x, y)\n",
    "        z = self.sample_latent(mean_z, logvar_z)\n",
    "        return self.decode(z, y), mean_z, logvar_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Copy the training code from section 6.4, and modify it for a conditional VAE**\n",
    "\n",
    "Hint: To train the conditional VAE we need to use one-hot encoding of the labels. You can use the following code for that:\n",
    "\n",
    "    y = F.one_hot(y,10).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train a conditional VAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_model = ConditionalVAE().to(device)\n",
    "train_cvae(cvae_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adapt the `plot_latent` function from section 6.5 for conditional VAEs and use your function to visualize the latent space for a few classes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do the latent dimensions represent? Is this the same for all labels?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adapt `scatterplot_latent` to show the distribution in the latent space.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How is this distribution different from the distribution of the VAE? Does this match the theory?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Would it be possible to classify digits based on the latent representation of the conditional VAE?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How would you use a conditional VAE to change the label of an image, while keeping everything else as similar as possible?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is the conditional VAE a strict improvement over the normal VAE in all cases?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare the latent representation vector $z$ in the VAE with the input for the generator in a GAN. Are they the same thing?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end\n",
    "\n",
    "Well done! Please double check the instructions at the top before you submit your results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
